{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clustering] Submission Akhir BMLP_Avira\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements advanced clustering techniques to analyze customer segmentation patterns using multiple algorithms and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, OPTICS\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset Shape: {df.shape}\")\n",
    "        print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "        return df\n",
    "    \n",
    "    def preprocess(self, df):\n",
    "        # Handle missing values using more robust methods\n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        # Scale features\n",
    "        df_scaled = self.scaler.fit_transform(df[numeric_cols])\n",
    "        return pd.DataFrame(df_scaled, columns=numeric_cols)\n",
    "\n",
    "# Initialize and process data\n",
    "preprocessor = DataPreprocessor()\n",
    "df = preprocessor.load_data('mall_customers.csv')\n",
    "df_processed = preprocessor.preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DimensionalityReducer:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        \n",
    "    def reduce_dimensions(self, data):\n",
    "        reduced_data = self.pca.fit_transform(data)\n",
    "        explained_variance = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        \n",
    "        # Visualize explained variance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'bo-')\n",
    "        plt.title('Cumulative Explained Variance Ratio')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return reduced_data\n",
    "\n",
    "reducer = DimensionalityReducer()\n",
    "data_reduced = reducer.reduce_dimensions(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Clustering Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ClusteringAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.best_k = None\n",
    "        \n",
    "    def find_optimal_k(self):\n",
    "        # Use KElbowVisualizer for more comprehensive analysis\n",
    "        visualizer = KElbowVisualizer(KMeans(), k=(2,12), metric='silhouette')\n",
    "        visualizer.fit(self.data)\n",
    "        self.best_k = visualizer.elbow_value_\n",
    "        plt.show()\n",
    "        return self.best_k\n",
    "    \n",
    "    def apply_clustering(self):\n",
    "        results = {}\n",
    "        \n",
    "        # K-Means\n",
    "        kmeans = KMeans(n_clusters=self.best_k, random_state=42)\n",
    "        results['kmeans'] = kmeans.fit_predict(self.data)\n",
    "        \n",
    "        # Agglomerative Clustering\n",
    "        agg = AgglomerativeClustering(n_clusters=self.best_k)\n",
    "        results['agglomerative'] = agg.fit_predict(self.data)\n",
    "        \n",
    "        # OPTICS - density-based clustering\n",
    "        optics = OPTICS(min_samples=5)\n",
    "        results['optics'] = optics.fit_predict(self.data)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_clusters(self, results):\n",
    "        for method, labels in results.items():\n",
    "            if len(np.unique(labels)) > 1:  # Check if more than one cluster\n",
    "                sil_score = silhouette_score(self.data, labels)\n",
    "                cal_score = calinski_harabasz_score(self.data, labels)\n",
    "                print(f\"\\n{method.upper()} Clustering Metrics:\")\n",
    "                print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "                print(f\"Calinski-Harabasz Score: {cal_score:.3f}\")\n",
    "\n",
    "# Perform clustering analysis\n",
    "analyzer = ClusteringAnalyzer(data_reduced)\n",
    "optimal_k = analyzer.find_optimal_k()\n",
    "clustering_results = analyzer
