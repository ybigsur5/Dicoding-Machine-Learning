# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.metrics import silhouette_score
import joblib

"""
Name: Avira
Institution: DBS Bank Ltd. Singapore
Project: Bank Transaction Fraud Detection Clustering
"""

# 1. Load and EDA
# Load dataset
df = pd.read_csv('dataset_clustering_project.csv')

# Advanced EDA
def perform_eda():
    # Basic info
    print("Dataset Info:")
    print(df.info())
    print("\nDataset Head:")
    print(df.head())
    print("\nDescriptive Statistics:")
    print(df.describe())
    
    # Advanced visualizations
    plt.figure(figsize=(15,10))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Feature Correlation Matrix')
    plt.tight_layout()
    plt.show()
    
    # Distribution plots for numerical features
    numerical_cols = df.select_dtypes(include=['float64','int64']).columns
    for col in numerical_cols:
        plt.figure(figsize=(10,6))
        sns.histplot(data=df, x=col, kde=True)
        plt.title(f'Distribution of {col}')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

perform_eda()

# 2. Data Preprocessing
def preprocess_data():
    # Check missing values and duplicates
    print("Missing Values:", df.isnull().sum())
    print("Duplicates:", df.duplicated().sum())
    
    # Drop ID columns
    id_columns = ['TransactionID', 'AccountID', 'DeviceID', 'IPAddress', 'MerchantID']
    df_clean = df.drop(columns=id_columns)
    
    # Handle missing values
    df_clean = df_clean.fillna(df_clean.mean())
    
    # Handle outliers using IQR
    def remove_outliers(df, column):
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        df = df[(df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)]
        return df
    
    numerical_cols = df_clean.select_dtypes(include=['float64','int64']).columns
    for col in numerical_cols:
        df_clean = remove_outliers(df_clean, col)
    
    # Feature binning for Amount
    df_clean['Amount_Bin'] = pd.qcut(df_clean['Amount'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    
    # Encoding
    le = LabelEncoder()
    categorical_cols = df_clean.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        df_clean[col] = le.fit_transform(df_clean[col])
    
    # Scaling
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df_clean), columns=df_clean.columns)
    
    return df_scaled, df_clean, scaler, le

df_scaled, df_clean, scaler, le = preprocess_data()

# 3. Clustering Model
def build_clustering_model():
    # Elbow Method
    kmeans = KMeans(random_state=42)
    visualizer = KElbowVisualizer(kmeans, k=(2,10))
    visualizer.fit(df_scaled)
    optimal_k = visualizer.elbow_value_
    
    # K-Means Clustering
    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
    clusters = kmeans_final.fit_predict(df_scaled)
    
    # Silhouette Score
    silhouette_avg = silhouette_score(df_scaled, clusters)
    print(f"Silhouette Score: {silhouette_avg}")
    
    # PCA for visualization
    pca = PCA(n_components=2)
    df_pca = pca.fit_transform(df_scaled)
    
    # Visualize clusters
    plt.figure(figsize=(10,8))
    scatter = plt.scatter(df_pca[:,0], df_pca[:,1], c=clusters, cmap='viridis')
    plt.title('Cluster Visualization using PCA')
    plt.colorbar(scatter)
    plt.show()
    
    # Save models
    joblib.dump(kmeans_final, "model_clustering.h5")
    joblib.dump(pca, "PCA_model_clustering.h5")
    
    return clusters, pca

clusters, pca = build_clustering_model()

# 4. Cluster Interpretation
def interpret_clusters():
    df_clean['Target'] = clusters
    
    # Analyze clusters
    cluster_analysis = df_clean.groupby('Target').agg({
        'Amount': ['mean', 'min', 'max'],
        'TransactionTime': ['mean', 'min', 'max']
    }).round(2)
    
    print("Cluster Analysis:")
    print(cluster_analysis)
    
    # Inverse transform
    df_original = df_clean.copy()
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        df_original[col] = le.inverse_transform(df_clean[col].astype(int))
    
    numerical_cols = df.select_dtypes(include=['float64','int64']).columns
    df_original[numerical_cols] = scaler.inverse_transform(df_clean[numerical_cols])
    
    # Save results
    df_original.to_csv('data_clustering_inverse.csv', index=False)
    df_clean.to_csv('data_clustering.csv', index=False)

interpret_clusters()
