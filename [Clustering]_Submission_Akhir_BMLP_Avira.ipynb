{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ybigsur5/Dicoding-Machine-Learning/blob/main/%5BClustering%5D_Submission_Akhir_BMLP_Avira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ],
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset **tanpa label** dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n",
        "   \n",
        "2. **Ketentuan Dataset**:\n",
        "   - **Tanpa label**: Dataset tidak boleh memiliki label atau kelas.\n",
        "   - **Jumlah Baris**: Minimal 1000 baris untuk memastikan dataset cukup besar untuk analisis yang bermakna.\n",
        "   - **Tipe Data**: Harus mengandung data **kategorikal** dan **numerikal**.\n",
        "     - *Kategorikal*: Misalnya jenis kelamin, kategori produk.\n",
        "     - *Numerikal*: Misalnya usia, pendapatan, harga.\n",
        "\n",
        "3. **Pembatasan**:  \n",
        "   Dataset yang sudah digunakan dalam latihan clustering (seperti customer segmentation) tidak boleh digunakan."
      ],
      "metadata": {
        "id": "hssSDn-5n3HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Library**"
      ],
      "metadata": {
        "id": "fKADPWcFKlj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning."
      ],
      "metadata": {
        "id": "LgA3ERnVn84N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Memuat Dataset**"
      ],
      "metadata": {
        "id": "f3YIEnAFKrKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut."
      ],
      "metadata": {
        "id": "Ey3ItwTen_7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"bank_transactions_data_2.csv\"  # Sesuaikan path jika berbeda\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "GHCGNTyrM5fS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "ca295bb4-c9be-461f-d856-0119b19e9032"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'bank_transactions_data_2.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-496553816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bank_transactions_data_2.csv\"\u001b[0m  \u001b[0;31m# Sesuaikan path jika berbeda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank_transactions_data_2.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset. EDA bertujuan untuk:\n",
        "\n",
        "1. **Memahami Struktur Data**\n",
        "   - Tinjau jumlah baris dan kolom dalam dataset.  \n",
        "   - Tinjau jenis data di setiap kolom (numerikal atau kategorikal).\n",
        "\n",
        "2. **Menangani Data yang Hilang**  \n",
        "   - Identifikasi dan analisis data yang hilang (*missing values*). Tentukan langkah-langkah yang diperlukan untuk menangani data yang hilang, seperti pengisian atau penghapusan data tersebut.\n",
        "\n",
        "3. **Analisis Distribusi dan Korelasi**  \n",
        "   - Analisis distribusi variabel numerik dengan statistik deskriptif dan visualisasi seperti histogram atau boxplot.  \n",
        "   - Periksa hubungan antara variabel menggunakan matriks korelasi atau scatter plot.\n",
        "\n",
        "4. **Visualisasi Data**  \n",
        "   - Buat visualisasi dasar seperti grafik distribusi dan diagram batang untuk variabel kategorikal.  \n",
        "   - Gunakan heatmap atau pairplot untuk menganalisis korelasi antar variabel.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ],
      "metadata": {
        "id": "bgZkbJLpK9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Memahami Struktur Data\n",
        "print(\"ðŸ”¹ Informasi Dataset\")\n",
        "print(df.info())\n",
        "print(\"\\nðŸ”¹ Dimensi Dataset:\", df.shape)"
      ],
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mengecek Data yang Hilang\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values_percentage = (missing_values / df.shape[0]) * 100\n",
        "missing_df = pd.DataFrame({\"Missing Values\": missing_values, \"Percentage\": missing_values_percentage})\n",
        "print(\"\\nðŸ”¹ Missing Values:\\n\", missing_df[missing_df[\"Missing Values\"] > 0])"
      ],
      "metadata": {
        "id": "gQW_g-n8kQPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Statistik Deskriptif untuk Variabel Numerik\n",
        "print(\"\\nðŸ”¹ Statistik Deskriptif:\\n\", df.describe())"
      ],
      "metadata": {
        "id": "FXAyBVqTlK0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualisasi Distribusi Variabel Numerik\n",
        "df.hist(figsize=(12, 10), bins=30, edgecolor=\"k\")\n",
        "plt.suptitle(\"Histogram Distribusi Variabel Numerik\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pYPtkeeRmLQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Heatmap Korelasi Antar Variabel Numerik\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Heatmap Korelasi Variabel Numerik\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z6LdqubemS3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Diagram Batang untuk Variabel Kategorikal\n",
        "# Daftar fitur kategorikal yang akan divisualisasikan\n",
        "categorical_features = [\"TransactionType\", \"Location\", \"Channel\", \"CustomerOccupation\"]\n",
        "\n",
        "# Buat subplots untuk menampilkan beberapa diagram batang\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop untuk setiap fitur kategorikal\n",
        "for i, col in enumerate(categorical_features):\n",
        "    sns.countplot(data=df, x=col, ax=axes[i], color=\"steelblue\")  # Warna seragam\n",
        "    axes[i].set_title(f\"Distribusi {col}\")\n",
        "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r7Uc1EKwmZ0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data Preprocessing**"
      ],
      "metadata": {
        "id": "cpgHfgnSK3ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah."
      ],
      "metadata": {
        "id": "COf8KUPXLg5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Menghapus Kolom yang Tidak Diperlukan\n",
        "columns_to_drop = [\"TransactionID\", \"AccountID\", \"DeviceID\", \"IP Address\", \"MerchantID\"]\n",
        "df.drop(columns=columns_to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "WzBGjcbQM7N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Menghapus Data Duplikat\n",
        "df_duplicates = df.duplicated().sum()\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "0es9Ni4gweKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Menangani Data Kosong (Missing Values)\n",
        "missing_values = df.isnull().sum()\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "0kxiJWQwwhG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Konversi Tipe Data\n",
        "df[\"TransactionDate\"] = pd.to_datetime(df[\"TransactionDate\"])\n",
        "df[\"PreviousTransactionDate\"] = pd.to_datetime(df[\"PreviousTransactionDate\"])\n",
        "df[\"CustomerAge\"] = pd.to_numeric(df[\"CustomerAge\"], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "RWK1Tvq4w3If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Normalisasi atau Standarisasi Fitur Numerik\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numerical_features = [\"TransactionAmount\", \"TransactionDuration\", \"AccountBalance\"]\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
      ],
      "metadata": {
        "id": "zDusvK0IxLMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Deteksi dan Penanganan Outlier (Menggunakan IQR Method)\n",
        "Q1 = df[numerical_features].quantile(0.25)\n",
        "Q3 = df[numerical_features].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Menentukan batas bawah dan atas untuk outlier\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Menghapus outlier dari dataset\n",
        "df_clean = df[~((df[numerical_features] < lower_bound) | (df[numerical_features] > upper_bound)).any(axis=1)]"
      ],
      "metadata": {
        "id": "eRsQ7SQoxVr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Encoding Data Kategorikal (Menggunakan One-Hot Encoding)\n",
        "categorical_features = [\"TransactionType\", \"Location\", \"Channel\", \"CustomerOccupation\"]\n",
        "df_encoded = pd.get_dummies(df_clean, columns=categorical_features, drop_first=True)\n",
        "\n",
        "df_encoded.shape, df_duplicates, missing_values.sum(), df_clean.shape"
      ],
      "metadata": {
        "id": "5R2DjsL6xeF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan tidak ada kolom bertipe datetime dalam dataset\n",
        "df_encoded = df_encoded.select_dtypes(include=['number'])  # Hanya pilih kolom numerik\n",
        "\n",
        "# Cek kembali tipe data"
      ],
      "metadata": {
        "id": "fsMtF5bLIUsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.dtypes)"
      ],
      "metadata": {
        "id": "Xb2_6_9hIN3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Pembangunan Model Clustering**"
      ],
      "metadata": {
        "id": "BR73dCnrLEiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **a. Pembangunan Model Clustering**"
      ],
      "metadata": {
        "id": "Fkd_QHXWMBDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, Anda membangun model clustering dengan memilih algoritma yang sesuai untuk mengelompokkan data berdasarkan kesamaan. Berikut adalah **rekomendasi** tahapannya.\n",
        "1. Pilih algoritma clustering yang sesuai.\n",
        "2. Latih model dengan data menggunakan algoritma tersebut."
      ],
      "metadata": {
        "id": "Kn6Y2qbqMVLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menggunakan K-Means Clusttering"
      ],
      "metadata": {
        "id": "wZz0WZuBKNmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan jumlah cluster (best_k) berdasarkan analisis sebelumnya\n",
        "best_k = 2  # Gantilah dengan jumlah cluster optimal yang telah ditemukan\n",
        "\n",
        "# Inisialisasi dan latih model K-Means\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(df_encoded)\n",
        "\n",
        "# Simpan label hasil clustering ke dalam dataframe\n",
        "df_encoded[\"Cluster\"] = labels\n",
        "\n",
        "# Tampilkan jumlah data di setiap cluster\n",
        "print(df_encoded[\"Cluster\"].value_counts())"
      ],
      "metadata": {
        "id": "q44G9q0CLoDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b. Evaluasi Model Clustering**"
      ],
      "metadata": {
        "id": "zsGVwzPKMEvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menentukan jumlah cluster yang optimal dalam model clustering, Anda dapat menggunakan metode Elbow atau Silhouette Score.\n",
        "\n",
        "Metode ini membantu kita menemukan jumlah cluster yang memberikan pemisahan terbaik antar kelompok data, sehingga model yang dibangun dapat lebih efektif. Berikut adalah **rekomendasi** tahapannya.\n",
        "1. Gunakan Silhouette Score dan Elbow Method untuk menentukan jumlah cluster optimal.\n",
        "2. Hitung Silhouette Score sebagai ukuran kualitas cluster."
      ],
      "metadata": {
        "id": "qk63ew39MeBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan rentang jumlah cluster\n",
        "inertia = []\n",
        "K_range = range(2, 11)  # Coba dari 2 hingga 10 cluster\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(df_encoded)  # Latih model K-Means\n",
        "    inertia.append(kmeans.inertia_)  # Simpan nilai inertia\n",
        "\n",
        "# Plot Elbow Method\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, inertia, marker='o', linestyle='--', color='b', label=\"Inertia\")\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hgYvwWOzM93L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan rentang jumlah cluster\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)  # Coba dari 2 hingga 10 cluster\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(df_encoded)\n",
        "    score = silhouette_score(df_encoded, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "    # Print Silhouette Score untuk tiap K\n",
        "    print(f\"Silhouette Score untuk K={k}: {score:.4f}\")\n",
        "\n",
        "# Plot Silhouette Scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, silhouette_scores, marker='s', linestyle='-', color='r', label=\"Silhouette Score\")\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Different K Values')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Menentukan K terbaik berdasarkan Silhouette Score tertinggi\n",
        "best_k = K_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "print(f\"\\nOptimal number of clusters berdasarkan Silhouette Score: {best_k}\")"
      ],
      "metadata": {
        "id": "0dUANK9cj6FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **c. Feature Selection (Opsional)**"
      ],
      "metadata": {
        "id": "vWZp5vKNQddd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silakan lakukan feature selection jika Anda membutuhkan optimasi model clustering. Jika Anda menerapkan proses ini, silakan lakukan pemodelan dan evaluasi kembali menggunakan kolom-kolom hasil feature selection. Terakhir, bandingkan hasil performa model sebelum dan sesudah menerapkan feature selection."
      ],
      "metadata": {
        "id": "QIHKgE07Q4c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan label hasil clustering sudah ada\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "df_encoded['cluster'] = kmeans.fit_predict(df_encoded)\n",
        "\n",
        "# Pisahkan fitur (X) dan target (y)\n",
        "X = df_encoded.drop(columns=['cluster'])  # Fitur\n",
        "y = df_encoded['cluster']  # Label hasil clustering\n",
        "\n",
        "# Pilih 5 fitur terbaik berdasarkan ANOVA F-test\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Tampilkan fitur yang terpilih\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(f\"Fitur terbaik yang dipilih: {selected_features}\")"
      ],
      "metadata": {
        "id": "CDYtetWtK9A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gunakan hanya fitur terbaik untuk clustering\n",
        "X_selected = df_encoded[selected_features]\n",
        "\n",
        "# Ulangi clustering dengan fitur terpilih\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X_selected)\n",
        "\n",
        "# Evaluasi dengan silhouette score\n",
        "silhouette_avg = silhouette_score(X_selected, labels)\n",
        "print(f\"Silhouette Score setelah Feature Selection: {silhouette_avg:.4f}\")"
      ],
      "metadata": {
        "id": "TUebEVGHLZCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduksi dimensi dengan PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_selected)\n",
        "\n",
        "# Clustering dengan data hasil PCA\n",
        "kmeans_pca = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "labels_pca = kmeans_pca.fit_predict(X_pca)\n",
        "\n",
        "# Evaluasi ulang silhouette score\n",
        "silhouette_pca = silhouette_score(X_pca, labels_pca)\n",
        "print(f\"Silhouette Score setelah PCA: {silhouette_pca:.4f}\")"
      ],
      "metadata": {
        "id": "5RL3uVJPLo4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inertia untuk Elbow Method\n",
        "inertia = []\n",
        "K_range = range(2, 11)  # Uji K dari 2 hingga 10\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_pca)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow Method\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, inertia, marker='o', linestyle='--', color='b', label=\"Inertia\")\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method After Feature Selection & PCA' if use_pca else 'Elbow Method After Feature Selection')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d-RRhcMGTl6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi silhouette score untuk setiap K\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    score = silhouette_score(X_pca, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "    # Print Silhouette Score untuk tiap K\n",
        "    print(f\"Silhouette Score untuk K={k}: {score:.4f}\")\n",
        "\n",
        "# Plot Silhouette Score\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, silhouette_scores, marker='s', linestyle='-', color='r', label=\"Silhouette Score\")\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score After Feature Selection & PCA' if use_pca else 'Silhouette Score After Feature Selection')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan K terbaik berdasarkan Silhouette Score\n",
        "best_k = K_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "print(f\"Jumlah cluster optimal berdasarkan Silhouette Score: {best_k}\")"
      ],
      "metadata": {
        "id": "0iUdkXenWyho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **d. Visualisasi Hasil Clustering**"
      ],
      "metadata": {
        "id": "nn01TKkLLRiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah model clustering dilatih dan jumlah cluster optimal ditentukan, langkah selanjutnya adalah menampilkan hasil clustering melalui visualisasi.\n",
        "\n",
        "Berikut adalah **rekomendasi** tahapannya.\n",
        "1. Tampilkan hasil clustering dalam bentuk visualisasi, seperti grafik scatter plot atau 2D PCA projection."
      ],
      "metadata": {
        "id": "gaz0fnhhMkRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Warna untuk cluster\n",
        "colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'cyan', 'magenta']\n",
        "n_clusters = len(set(labels_pca))\n",
        "\n",
        "# Plot hasil clustering\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(X_pca[labels_pca == i, 0], X_pca[labels_pca == i, 1],\n",
        "                s=50, c=colors[i], label=f'Cluster {i}')\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Cluster Visualization')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lfOjVvfYM-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot untuk melihat distribusi cluster pada beberapa fitur\n",
        "sns.pairplot(df_encoded, hue=\"Cluster\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZxoVzsQfXdMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **e. Analisis dan Interpretasi Hasil Cluster**"
      ],
      "metadata": {
        "id": "X4eydPWJLH4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretasi Target"
      ],
      "metadata": {
        "id": "mxTeTkTkA7o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial: Melakukan Inverse Transform pada Data Target Setelah Clustering**\n",
        "\n",
        "Setelah melakukan clustering dengan model **KMeans**, kita perlu mengembalikan data yang telah diubah (normalisasi, standarisasi, atau label encoding) ke bentuk aslinya. Berikut adalah langkah-langkahnya.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Tambahkan Hasil Label Cluster ke DataFrame**\n",
        "Setelah mendapatkan hasil clustering, kita tambahkan label cluster ke dalam DataFrame yang telah dinormalisasi.\n",
        "\n",
        "```python\n",
        "df_normalized['Cluster'] = model_kmeans.labels_\n",
        "```\n",
        "\n",
        "Lakukan Inverse Transform pada feature yang sudah dilakukan Labelisasi dan Standararisasi. Berikut code untuk melakukannya:\n",
        "label_encoder.inverse_transform(X_Selected[['Fitur']])\n",
        "\n",
        "Lalu masukkan ke dalam kolom dataset asli atau membuat dataframe baru\n",
        "```python\n",
        "df_normalized['Fitur'] = label_encoder.inverse_transform(df_normalized[['Fitur']])\n",
        "```\n",
        "Masukkan Data yang Sudah Di-Inverse ke dalam Dataset Asli atau Buat DataFrame Baru\n",
        "```python\n",
        "df_original['Fitur'] = df_normalized['Fitur']\n",
        "```"
      ],
      "metadata": {
        "id": "NJDsBARmIsbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_normalized = df_encoded.copy()"
      ],
      "metadata": {
        "id": "ySc78cHdOiaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_normalized = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "df_normalized['Cluster'] = kmeans_pca.labels_\n"
      ],
      "metadata": {
        "id": "2mMIWVIP1xI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inverse Data Jika Melakukan Normalisasi/Standardisasi"
      ],
      "metadata": {
        "id": "vITTtdoT_fIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse Transform untuk Data yang Distandarisasi\n",
        "Jika data numerik telah dinormalisasi menggunakan StandardScaler atau MinMaxScaler, kita bisa mengembalikannya ke skala asli:\n",
        "```python\n",
        "df_normalized[['Fitur_Numerik']] = scaler.inverse_transform(df_normalized[['Fitur_Numerik']])\n",
        "```"
      ],
      "metadata": {
        "id": "bUGkyAvnKRrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat statistik deskriptif tiap cluster\n",
        "cluster_summary = df_normalized.groupby('Cluster')[['PC1', 'PC2']].describe()\n",
        "print(cluster_summary)"
      ],
      "metadata": {
        "id": "u9Q3YpDg7glZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warna untuk tiap cluster\n",
        "colors = ['blue', 'orange']\n",
        "\n",
        "# Plot hasil clustering berdasarkan PCA\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x=df_normalized['PC1'],\n",
        "    y=df_normalized['PC2'],\n",
        "    hue=df_normalized['Cluster'],\n",
        "    palette=colors,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# Tambahkan label dan judul\n",
        "plt.xlabel('Principal Component 1 (PC1)')\n",
        "plt.ylabel('Principal Component 2 (PC2)')\n",
        "plt.title('Visualisasi Clustering dengan PCA')\n",
        "plt.legend(title='Cluster')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6zfcnHHwFZXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan dataset asli sebelum normalisasi tersedia\n",
        "df_original = df.copy()  # Gunakan salinan dataset sebelum PCA\n",
        "\n",
        "# Menambahkan kolom 'Cluster' dari hasil clustering\n",
        "df_original['Cluster'] = df_normalized['Cluster']\n",
        "\n",
        "# Melihat jumlah anggota tiap cluster\n",
        "print(df_original['Cluster'].value_counts())\n"
      ],
      "metadata": {
        "id": "sHDonTySGru1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat statistik deskriptif per cluster untuk fitur numerik\n",
        "cluster_summary = df_original.groupby('Cluster').describe()\n",
        "print(cluster_summary)"
      ],
      "metadata": {
        "id": "RshB1y1-Pxvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_original.dtypes)"
      ],
      "metadata": {
        "id": "saKRdh4EQI5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = df_original.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(\"Fitur Numerik dalam Dataset:\", numerical_features.tolist())"
      ],
      "metadata": {
        "id": "YPhoik9dSsyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = df_original.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Fitur kategorikal dalam dataset:\", categorical_features.tolist())"
      ],
      "metadata": {
        "id": "ZMoe-fQ8QkeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_features:\n",
        "    print(f\"\\nFitur: {col}\")\n",
        "    print(df_original[col].value_counts())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jdo20ZhYQqr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat distribusi fitur kategorikal berdasarkan cluster\n",
        "categorical_features = ['TransactionType', 'Location', 'Channel', 'CustomerOccupation']  # Ganti dengan nama fitur kategorikal yang ada\n",
        "\n",
        "for col in categorical_features:\n",
        "    print(f\"\\nDistribusi {col} per Cluster:\")\n",
        "    print(df_original.groupby('Cluster')[col].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "PQ6dZP8_Q0eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['TransactionAmount', 'CustomerAge', 'TransactionDuration', 'LoginAttempts', 'AccountBalance', 'Cluster']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_features, 1):\n",
        "    plt.subplot(1, len(numerical_features), i)\n",
        "    sns.boxplot(x='Cluster', y=col, data=df_original)\n",
        "    plt.title(f'Boxplot {col} per Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BC77Zfu1RGEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah melakukan clustering, langkah selanjutnya adalah menganalisis karakteristik dari masing-masing cluster berdasarkan fitur yang tersedia.\n",
        "\n",
        "Berikut adalah **rekomendasi** tahapannya.\n",
        "1. Analisis karakteristik tiap cluster berdasarkan fitur yang tersedia (misalnya, distribusi nilai dalam cluster).\n",
        "2. Berikan interpretasi: Apakah hasil clustering sesuai dengan ekspektasi dan logika bisnis? Apakah ada pola tertentu yang bisa dimanfaatkan?"
      ],
      "metadata": {
        "id": "SENfLnfRMpC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tulis hasil interpretasinya di sini.\n",
        "1. Cluster 1:\n",
        "2. Cluster 2:\n",
        "3. Cluster 3:"
      ],
      "metadata": {
        "id": "JfgVMEBDS3KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interpretasi Hasil Clustering**\n",
        "\n",
        "Berdasarkan hasil clustering menggunakan **PCA + K-Means**, berikut adalah analisis karakteristik dari masing-masing cluster berdasarkan fitur kategorikal dan numerikal.\n",
        "\n",
        "---\n",
        "\n",
        "## **Cluster 0**  \n",
        "\n",
        "### **1. Tipe Transaksi**  \n",
        "- **Cluster 0** lebih dominan dalam melakukan transaksi **debit (77.18%)** dibanding kredit.  \n",
        "- Kebiasaan ini menunjukkan bahwa pelanggan dalam cluster ini lebih sering menggunakan uang langsung dari saldo mereka, bukan dari kredit.  \n",
        "\n",
        "### **2. Lokasi Pelanggan**  \n",
        "- Pelanggan dalam cluster ini sebagian besar berasal dari **Austin, Charlotte, Los Angeles, dan New York**.  \n",
        "- Ini menunjukkan bahwa cluster ini lebih terkonsentrasi di kota-kota besar dengan populasi tinggi.  \n",
        "\n",
        "### **3. Kanal Transaksi**  \n",
        "- Mayoritas transaksi di cluster ini terjadi di **Branch (35.8%)**, diikuti oleh **ATM (33.17%)**, dan **Online (31.03%)**.  \n",
        "- Ini menandakan bahwa pelanggan di cluster ini lebih cenderung melakukan transaksi langsung di cabang bank dibandingkan menggunakan layanan digital atau ATM.  \n",
        "\n",
        "### **4. Pekerjaan Pelanggan**  \n",
        "- Cluster ini didominasi oleh **Doctor (25.78%)**, **Engineer (25.68%)**, dan **Student (24.63%)**.  \n",
        "- Banyaknya dokter dan insinyur di cluster ini menunjukkan bahwa pelanggan dalam kelompok ini sebagian besar adalah profesional yang aktif bekerja.  \n",
        "\n",
        "### **5. Analisis Singkat**  \n",
        "- **Cluster 0 merupakan segmen profesional aktif** dengan kecenderungan melakukan transaksi langsung di cabang.  \n",
        "- Strategi pemasaran yang cocok untuk cluster ini adalah **layanan premium, program loyalitas, atau kartu kredit dengan benefit khusus untuk profesional**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Cluster 1**  \n",
        "\n",
        "### **1. Tipe Transaksi**  \n",
        "- **Cluster 1** juga lebih sering menggunakan **debit (77.90%)** dibanding kredit, mirip dengan Cluster 0.  \n",
        "- Namun, ada sedikit perbedaan dalam proporsi pengguna kartu kredit, yang lebih rendah dibanding Cluster 0.  \n",
        "\n",
        "### **2. Lokasi Pelanggan**  \n",
        "- Sebagian besar pelanggan dalam cluster ini berasal dari **Portland, Virginia Beach, El Paso, dan Austin**.  \n",
        "- Ini menunjukkan bahwa pelanggan dalam cluster ini tersebar di berbagai kota, tidak hanya terpusat di kota-kota besar seperti Cluster 0.  \n",
        "\n",
        "### **3. Kanal Transaksi**  \n",
        "- Penggunaan kanal transaksi lebih merata antara **ATM (33.53%)**, **Branch (33.45%)**, dan **Online (33.02%)**.  \n",
        "- Ini menandakan bahwa pelanggan dalam cluster ini memiliki preferensi transaksi yang lebih fleksibel dibanding Cluster 0.  \n",
        "\n",
        "### **4. Pekerjaan Pelanggan**  \n",
        "- Cluster ini didominasi oleh **Student (27.68%)**, **Retired (24.30%)**, dan **Doctor (21.68%)**.  \n",
        "- Adanya banyak mahasiswa dan pensiunan di cluster ini menunjukkan bahwa pelanggan dalam kelompok ini memiliki kebiasaan keuangan yang berbeda dari profesional di Cluster 0.  \n",
        "\n",
        "### **5. Analisis Singkat**  \n",
        "- **Cluster 1 adalah segmen pelanggan yang lebih fleksibel dalam memilih kanal transaksi dan terdiri dari lebih banyak mahasiswa serta pensiunan**.  \n",
        "- Strategi pemasaran yang cocok untuk cluster ini adalah **program cashback untuk transaksi online atau ATM, serta promosi khusus untuk mahasiswa dan pensiunan**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Kesimpulan Akhir**  \n",
        "- **Cluster 0** berisi **profesional aktif yang lebih sering melakukan transaksi di cabang bank dan berasal dari kota-kota besar**.  \n",
        "- **Cluster 1** berisi **mahasiswa dan pensiunan dengan kebiasaan transaksi lebih fleksibel dan tersebar di berbagai kota**.  \n",
        "- **Tidak ada perbedaan signifikan dalam jumlah transaksi, durasi transaksi, dan saldo akun antara kedua cluster**.  \n",
        "\n",
        "> **Rekomendasi Strategi Pemasaran:**  \n",
        "> - **Cluster 0** â†’ **Layanan premium, program loyalitas, kartu kredit dengan benefit khusus untuk profesional**.  \n",
        "> - **Cluster 1** â†’ **Program cashback untuk transaksi online atau ATM, serta promosi khusus untuk mahasiswa dan pensiunan**.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hOygRVqs_vdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Mengeksport Data**\n",
        "\n",
        "Simpan hasilnya ke dalam file CSV."
      ],
      "metadata": {
        "id": "jaYP1fx5VgWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan dataset hasil clustering ke dalam file CSV\n",
        "df_original.to_csv('hasil_clustering.csv', index=False)\n",
        "\n",
        "print(\"Dataset berhasil disimpan sebagai 'hasil_clustering.csv'\")"
      ],
      "metadata": {
        "id": "fkbg_o80aRSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}